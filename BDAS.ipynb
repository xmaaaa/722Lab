{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d9068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|customerID|gender|ZipCode|SeniorCitizen|Partner|Dependents|tenure|PhoneService|   MultipleLines|InternetService|     OnlineSecurity|       OnlineBackup|   DeviceProtection|        TechSupport|        StreamingTV|    StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
      "+----------+------+-------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|7590-VHVEG|Female| 4584.0|          0.0|    Yes|        No|   1.0|          No|No phone service|            DSL|                 No|                Yes|                 No|                 No|                 No|                 No|Month-to-month|             Yes|    Electronic check|         29.85|       29.85|   No|\n",
      "|5575-GNVDE|  Male| 6485.0|          0.0|     No|        No|  34.0|         Yes|              No|            DSL|                Yes|                 No|                Yes|                 No|                 No|                 No|      One year|              No|        Mailed check|         56.95|      1889.5|   No|\n",
      "|3668-QPYBK|  Male| 5423.0|          0.0|     No|        No|   2.0|         Yes|              No|            DSL|                Yes|                Yes|                 No|                 No|                 No|                 No|Month-to-month|             Yes|        Mailed check|         53.85|      108.15|  Yes|\n",
      "|7795-CFOCW|  Male| 9023.0|          0.0|     No|        No|  45.0|          No|No phone service|            DSL|                Yes|                 No|                Yes|                Yes|                 No|                 No|      One year|              No|Bank transfer (au...|          42.3|     1840.75|   No|\n",
      "|9237-HQITU|Female| 2490.0|          0.0|     No|        No|   2.0|         Yes|              No|    Fiber optic|                 No|                 No|                 No|                 No|                 No|                 No|Month-to-month|             Yes|    Electronic check|          70.7|      151.65|  Yes|\n",
      "|9305-CDSKC|Female| 9364.0|          0.0|     No|        No|   8.0|         Yes|             Yes|    Fiber optic|                 No|                 No|                Yes|                 No|                Yes|                Yes|Month-to-month|             Yes|    Electronic check|         99.65|       820.5|  Yes|\n",
      "|1452-KIOVK|  Male| 8157.0|          0.0|     No|       Yes|  22.0|         Yes|             Yes|    Fiber optic|                 No|                Yes|                 No|                 No|                Yes|                 No|Month-to-month|             Yes|Credit card (auto...|          89.1|      1949.4|   No|\n",
      "|6713-OKOMC|Female| 2374.0|          0.0|     No|        No|  10.0|          No|No phone service|            DSL|                Yes|                 No|                 No|                 No|                 No|                 No|Month-to-month|              No|        Mailed check|         29.75|       301.9|   No|\n",
      "|7892-POOKP|Female| 9575.0|          0.0|    Yes|        No|  28.0|         Yes|             Yes|    Fiber optic|                 No|                 No|                Yes|                Yes|                Yes|                Yes|Month-to-month|             Yes|    Electronic check|         104.8|     3046.05|  Yes|\n",
      "|6388-TABGU|  Male| 5265.0|          0.0|     No|       Yes|  62.0|         Yes|              No|            DSL|                Yes|                Yes|                 No|                 No|                 No|                 No|      One year|              No|Bank transfer (au...|         56.15|     3487.95|   No|\n",
      "|9763-GRSKD|  Male| 1736.0|          0.0|    Yes|       Yes|  13.0|         Yes|              No|            DSL|                Yes|                 No|                 No|                 No|                 No|                 No|Month-to-month|             Yes|        Mailed check|         49.95|      587.45|   No|\n",
      "|7469-LKBCI|  Male| 1291.0|          0.0|     No|        No|  16.0|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|              No|Credit card (auto...|         18.95|       326.8|   No|\n",
      "|8091-TTVAX|  Male| 7801.0|          0.0|    Yes|        No|  58.0|         Yes|             Yes|    Fiber optic|                 No|                 No|                Yes|                 No|                Yes|                Yes|      One year|              No|Credit card (auto...|        100.35|      5681.1|   No|\n",
      "|0280-XJGEX|  Male| 2578.0|          0.0|     No|        No|  49.0|         Yes|             Yes|    Fiber optic|                 No|                Yes|                Yes|                 No|                Yes|                Yes|Month-to-month|             Yes|Bank transfer (au...|         103.7|      5036.3|  Yes|\n",
      "|5129-JLPIS|  Male| 5904.0|          0.0|     No|        No|  25.0|         Yes|              No|    Fiber optic|                Yes|                 No|                Yes|                Yes|                Yes|                Yes|Month-to-month|             Yes|    Electronic check|         105.5|     2686.05|   No|\n",
      "|3655-SNQYZ|Female| 2030.0|          0.0|    Yes|       Yes|  69.0|         Yes|             Yes|    Fiber optic|                Yes|                Yes|                Yes|                Yes|                Yes|                Yes|      Two year|              No|Credit card (auto...|        113.25|     7895.15|   No|\n",
      "|8191-XWSZG|Female| 3486.0|          0.0|     No|        No|  52.0|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      One year|              No|        Mailed check|         20.65|     1022.95|   No|\n",
      "|9959-WOFKT|  Male| 4989.0|          0.0|     No|       Yes|  71.0|         Yes|             Yes|    Fiber optic|                Yes|                 No|                Yes|                 No|                Yes|                Yes|      Two year|              No|Bank transfer (au...|         106.7|     7382.25|   No|\n",
      "|4190-MFLUW|Female| 4017.0|          0.0|    Yes|       Yes|  10.0|         Yes|              No|            DSL|                 No|                 No|                Yes|                Yes|                 No|                 No|Month-to-month|              No|Credit card (auto...|          55.2|      528.35|  Yes|\n",
      "|4183-MYFRB|Female| 3402.0|          0.0|     No|        No|  21.0|         Yes|              No|    Fiber optic|                 No|                Yes|                Yes|                 No|                 No|                Yes|Month-to-month|             Yes|    Electronic check|         90.05|      1862.9|   No|\n",
      "+----------+------+-------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "(7043, 22)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# 01-BU\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Describe the business objectives here\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 02-DU\n",
    "\n",
    "# Load Dataset\n",
    "\n",
    "file = 'WA_Fn-UseC_-Telco-Customer-Churn.xlsx'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExcelFileReader\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the Excel file\n",
    "df = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file)\n",
    "\n",
    "# Show the first few rows and the shape of the DataFrame\n",
    "df.show()\n",
    "print((df.count(), len(df.columns)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e66a139",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/compat/_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/excel/_base.py:457\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    456\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/excel/_base.py:1419\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[0;32m-> 1419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/excel/_openpyxl.py:524\u001b[0m, in \u001b[0;36mOpenpyxlReader.__init__\u001b[0;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    511\u001b[0m     filepath_or_buffer: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[1;32m    512\u001b[0m     storage_options: StorageOptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03m    Reader using openpyxl engine.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m        passed to fsspec for appropriate URLs (see ``_get_filepath_or_buffer``)\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenpyxl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(filepath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/compat/_optional.py:141\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl."
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_excel(file)\n",
    "print(df.head())\n",
    "df.shape\n",
    "df.info()\n",
    "df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Explore Data\n",
    "\n",
    "df_desc=df.describe()\n",
    "round(df_desc,2)\n",
    "round(df_desc.transpose(),2)\n",
    "\n",
    "df['InternetService'].describe()\n",
    "\n",
    "internet_service_counts = df['InternetService'].value_counts()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.pie(internet_service_counts, labels=internet_service_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Internet Service Distribution')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df['MonthlyCharges'].plot.hist(bins=50)\n",
    "plt.xlabel('Monthly Charges')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Monthly Charges')\n",
    "\n",
    "\n",
    "df['TotalCharges'].plot.hist(bins=50)\n",
    "plt.xlabel('Total Charges')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Total Charges')\n",
    "\n",
    "df['tenure'].plot.hist(bins=50)\n",
    "plt.xlabel('Tenure')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Tenure')\n",
    "\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "total = churn_counts.sum()\n",
    "labels_with_counts = [f'{label}\\n{count} ({count/total:.1%})' for label, count in zip(churn_counts.index, churn_counts)]\n",
    "plt.pie(churn_counts, labels=labels_with_counts, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Internet Service Distribution')\n",
    "plt.show()\n",
    "\n",
    "null_counts = df.isna().sum()\n",
    "print(\"Null Counts:\")\n",
    "print(null_counts)\n",
    "\n",
    "contract_counts = df['Contract'].value_counts()\n",
    "contract_counts.plot.bar()\n",
    "OnlineSecurity_counts = df['OnlineSecurity'].value_counts()\n",
    "OnlineSecurity_counts.plot.bar()\n",
    "Churn_counts = df['Churn'].value_counts()\n",
    "Churn_counts.plot.bar()\n",
    "\n",
    "\n",
    "# 03-DP\n",
    "\n",
    "# Add any pre-processing steps\n",
    "\n",
    "# Change numeric to binary\n",
    "\n",
    "# Dropping specific columns\n",
    "columns_to_drop = ['ZipCode','customerID','gender','SeniorCitizen','Partner']\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Define a function to check if a value is blank or null\n",
    "def is_blank_or_null(value):\n",
    "    return pd.isna(value) or (isinstance(value, str) and len(value.strip()) == 0)\n",
    "\n",
    "# Filter the dataset based on the given conditions\n",
    "df_cleaned = df_cleaned[\n",
    "    ~df_cleaned.apply(lambda row: (\n",
    "        is_blank_or_null(row['DeviceProtection']) or\n",
    "        is_blank_or_null(row['TechSupport']) or\n",
    "        pd.isna(row['TotalCharges']) or\n",
    "        pd.isna(row['MonthlyCharges'])\n",
    "    ), axis=1)\n",
    "]\n",
    "null_counts = df_cleaned.isna().sum()\n",
    "print(\"Null Counts:\")\n",
    "print(null_counts)\n",
    "\n",
    "\n",
    "# Correcting inconsistencies in relevant columns\n",
    "df_cleaned['Churn'] = df_cleaned['Churn'].replace({1: 'Yes', 0: 'No'})\n",
    "df_cleaned['OnlineSecurity'] = df_cleaned['OnlineSecurity'].replace({'Yes1': 'Yes', 'No2': 'No'})\n",
    "df_cleaned['PaymentMethod'] = df_cleaned['PaymentMethod'].replace({\n",
    "    'Electronic check111': 'Electronic check', 'Bank transfer (automatic)222': 'Bank transfer (automatic)'})\n",
    "df_cleaned['Contract'] = df_cleaned['Contract'].replace({\n",
    "    'One year3224': 'One year',\n",
    "    'Two year23': 'Two year'\n",
    "})\n",
    "\n",
    "contract_counts = df_cleaned['Contract'].value_counts()\n",
    "contract_counts.plot.bar()\n",
    "OnlineSecurity_counts = df_cleaned['OnlineSecurity'].value_counts()\n",
    "OnlineSecurity_counts.plot.bar()\n",
    "Churn_counts = df_cleaned['Churn'].value_counts()\n",
    "Churn_counts.plot.bar()\n",
    "Churn_counts = df_cleaned['PaymentMethod'].value_counts()\n",
    "Churn_counts.plot.bar()\n",
    "\n",
    "# Determine the 10th and 90th percentile thresholds for `MonthlyCharges` and `TotalCharges`\n",
    "monthly_charges_lower = df_cleaned['MonthlyCharges'].quantile(0.05)\n",
    "monthly_charges_upper = df_cleaned['MonthlyCharges'].quantile(0.95)\n",
    "total_charges_lower = df_cleaned['TotalCharges'].quantile(0.05)\n",
    "total_charges_upper = df_cleaned['TotalCharges'].quantile(0.95)\n",
    "\n",
    "# Filtering out rows outside these thresholds\n",
    "df_cleaned = df_cleaned[\n",
    "    (df_cleaned['MonthlyCharges'] >= monthly_charges_lower) & (df_cleaned['MonthlyCharges'] <= monthly_charges_upper) &\n",
    "    (df_cleaned['TotalCharges'] >= total_charges_lower) & (df_cleaned['TotalCharges'] <= total_charges_upper)\n",
    "]\n",
    "\n",
    "\n",
    "# Define columns representing individual services\n",
    "service_columns = [\n",
    "    'InternetService', 'StreamingTV', 'StreamingMovies', \n",
    "    'TechSupport', 'DeviceProtection', 'OnlineBackup', 'OnlineSecurity'\n",
    "]\n",
    "\n",
    "# Converting these columns to binary indicators (1 if 'Yes', 0 if 'No')\n",
    "for col in service_columns:\n",
    "    df_cleaned[col] = df_cleaned[col].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Create a new feature for total services used\n",
    "df_cleaned['TotalServicesUsed'] = df_cleaned[service_columns].sum(axis=1)\n",
    "\n",
    "# Define a function to group tenure\n",
    "def tenure_group(tenure):\n",
    "    if tenure <= 12:\n",
    "        return 0\n",
    "    elif tenure <= 24:\n",
    "        return 1\n",
    "    elif tenure <= 36:\n",
    "        return 2\n",
    "    elif tenure <= 48:\n",
    "        return 3\n",
    "    elif tenure <= 60:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# Apply the function to create a new column for tenure grouping\n",
    "df_cleaned['TenureGroup'] = df_cleaned['tenure'].apply(tenure_group)\n",
    "\n",
    "# Load the split datasets from Excel files\n",
    "data_part1 = pd.read_excel('/Users/xm/Documents/UOA/info722/Iteration2_project_files/WA_Fn-UseC_-Telco-Customer-Churn-dirty.xlsx')\n",
    "data_part2 = pd.read_excel('/Users/xm/Documents/UOA/info722/Iteration2_project_files/WA_Fn-UseC_-Telco-Customer-Churn-dirty2.xlsx')\n",
    "merged_data = pd.concat([data_part1, data_part2], axis=1)\n",
    "\n",
    "# Define label mapping for categorical columns\n",
    "label_mapping = {\n",
    "    'No': 0,\n",
    "    'Yes': 1,\n",
    "    'No internet service': 0,\n",
    "    'DSL': 1,\n",
    "    'Fiber optic': 2,\n",
    "    'Electronic check': 1,\n",
    "    'Mailed check': 2,\n",
    "    'Bank transfer (automatic)': 3,\n",
    "    'Credit card (automatic)': 4,\n",
    "    'Month-to-month': 0,\n",
    "    'One year': 1,\n",
    "    'Two year': 2,\n",
    "    'No phone service': 0  # Added mapping for MultipleLines\n",
    "}\n",
    "\n",
    "# Function to map categorical values to numerical labels if needed\n",
    "def safe_map(value, mapping):\n",
    "    return mapping.get(value, value) if isinstance(value, str) else value\n",
    "\n",
    "# Apply safe_map to relevant columns\n",
    "df_cleaned['Churn'] = df_cleaned['Churn'].apply(lambda x: safe_map(x, label_mapping))\n",
    "df_cleaned['PaymentMethod'] = df_cleaned['PaymentMethod'].apply(lambda x: safe_map(x, label_mapping))\n",
    "df_cleaned['Dependents'] = df_cleaned['Dependents'].apply(lambda x: safe_map(x, label_mapping))\n",
    "df_cleaned['PhoneService'] = df_cleaned['PhoneService'].apply(lambda x: safe_map(x, label_mapping))\n",
    "df_cleaned['MultipleLines'] = df_cleaned['MultipleLines'].apply(lambda x: safe_map(x, label_mapping))\n",
    "df_cleaned['Contract'] = df_cleaned['Contract'].apply(lambda x: safe_map(x, label_mapping))\n",
    "df_cleaned['PaperlessBilling'] = df_cleaned['PaperlessBilling'].apply(lambda x: safe_map(x, label_mapping))\n",
    "df_cleaned.info()\n",
    "\n",
    "# 04-DT\n",
    "\n",
    "# Add any transformation steps\n",
    "\n",
    "# One hot encoding for categorical\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "# Selecting all relevant columns\n",
    "categorical_features = ['InternetService', 'PaymentMethod', 'TenureGroup', 'StreamingTV', \n",
    "                        'StreamingMovies', 'TechSupport', 'DeviceProtection', 'OnlineBackup', \n",
    "                        'OnlineSecurity', 'Dependents', 'PhoneService', 'MultipleLines', \n",
    "                        'Contract', 'PaperlessBilling']\n",
    "numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges', 'TotalServicesUsed']\n",
    "\n",
    "# Chi-square test for categorical features\n",
    "chi2_vals, p_vals = chi2(pd.get_dummies(df_cleaned[categorical_features]), df_cleaned['Churn'])\n",
    "\n",
    "# Display chi-square results\n",
    "print(\"Chi-square results for categorical features:\")\n",
    "for feature, chi2_val, p_val in zip(categorical_features, chi2_vals, p_vals):\n",
    "    print(f\"Feature: {feature}, Chi2: {chi2_val:.4f}, P-value: {p_val:.4f}\")\n",
    "\n",
    "# Correlation matrix for numeric features\n",
    "corr_matrix = df_cleaned[numeric_features + ['Churn']].corr()\n",
    "\n",
    "# Display correlation matrix\n",
    "print(\"Correlation matrix:\\n\", corr_matrix)\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Train a model to get feature importance\n",
    "X = df_cleaned.drop(columns=['Churn'])\n",
    "y = df_cleaned['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Display feature importances\n",
    "print(\"Feature Importances:\\n\", feature_importances)\n",
    "\n",
    "columns_to_drop = ['Dependents', 'TotalServicesUsed','PhoneService', 'InternetService']\n",
    "df_cleaned = df_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "# Log transformation for skewed features\n",
    "df_cleaned['LogTotalCharges'] = np.log1p(df_cleaned['TotalCharges'])  # log1p to handle zeros\n",
    "df_cleaned['LogMonthlyCharges'] = np.log1p(df_cleaned['MonthlyCharges'])\n",
    "\n",
    "# Applying Box-Cox transformation\n",
    "pt = PowerTransformer(method='box-cox')\n",
    "df_cleaned[['LogTotalCharges', 'LogMonthlyCharges']] = pt.fit_transform(df_cleaned[['LogTotalCharges', 'LogMonthlyCharges']])\n",
    "\n",
    "df_cleaned['LogMonthlyCharges'].plot.hist(bins=50)\n",
    "plt.xlabel('LogMonthlyCharges Charges')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of LogMonthlyCharges')\n",
    "\n",
    "df_cleaned['LogTotalCharges'].plot.hist(bins=50)\n",
    "plt.xlabel('LogTotalCharges')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of LogTotalCharges')\n",
    "\n",
    "# Retain original numerical features and create standardized versions\n",
    "numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "standardized_numeric_features = ['Std_' + feat for feat in numeric_features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_cleaned[standardized_numeric_features] = scaler.fit_transform(df_cleaned[numeric_features])\n",
    "\n",
    "# 05-DMM\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Identify the Data Mining method\n",
    "Describe how it aligns with the objectives\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 06-DMA\n",
    "\n",
    "# Load relevant algorithms\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Prepare features for modeling\n",
    "X = df_cleaned.drop(columns=['Churn'])\n",
    "y = df_cleaned['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "    \n",
    "# 07-DM\n",
    "\n",
    "# Execute DM task\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "# Evaluate Different Splits\n",
    "splits = [(0.7, 0.3), (0.8, 0.2), (0.9, 0.1)]\n",
    "\n",
    "for train_size, test_size in splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    print(f\"Split: {int(train_size * 100)}/{int(test_size * 100)}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred))\n",
    "    print()\n",
    "\n",
    "# Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "# Feature Importance\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "# Visualize Feature Importance\n",
    "feature_importances.plot(kind='bar', title=\"Random Forest Feature Importance\")\n",
    "plt.show()\n",
    "# Visualize a Specific Tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(rf.estimators_[0], feature_names=X.columns, class_names=['No Churn', 'Churn'], filled=True)\n",
    "plt.title(\"Random Forest Tree Visualization\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Neural Network model\n",
    "nn = MLPClassifier(hidden_layer_sizes=(10, 10), activation='relu', \n",
    "                   learning_rate_init=0.01, max_iter=200, random_state=42)\n",
    "nn.fit(X_train, y_train)\n",
    "# Extract weights from the model\n",
    "input_to_hidden_weights = nn.coefs_[0]  # Weights from input to first hidden layer\n",
    "# Visualize the weights\n",
    "feature_influence = pd.DataFrame(input_to_hidden_weights, index=X.columns)\n",
    "feature_influence.sum(axis=1).sort_values(ascending=False).plot(kind='bar', title=\"Feature Influence from NN Weights\")\n",
    "plt.show()\n",
    "# Heatmap for Weights\n",
    "sns.heatmap(input_to_hidden_weights, cmap='coolwarm')\n",
    "plt.title(\"Neural Network Weights (Input to Hidden Layer)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                              max_depth=10, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "# Feature Importance\n",
    "importances = xgb_model.get_booster().get_score(importance_type='weight')\n",
    "# Convert to a pandas Series\n",
    "importances_series = pd.Series(importances).sort_values(ascending=False)\n",
    "print(\"XGBoost Feature Importance (by weight):\\n\", importances_series)\n",
    "# Visualize the feature importance\n",
    "importances_series.plot(kind='bar', title=\"XGBoost Feature Importance (Weight)\")\n",
    "plt.show()\n",
    "\n",
    "# Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=500, solver='liblinear', \n",
    "                             C=1.0, penalty='l2', random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "# Coefficients\n",
    "coefficients = pd.Series(log_reg.coef_[0], index=X.columns)\n",
    "print(\"Logistic Regression Coefficients:\\n\", coefficients)\n",
    "# Visualize Coefficients\n",
    "coefficients.plot(kind='bar', title=\"Logistic Regression Coefficients\")\n",
    "plt.show()\n",
    "\n",
    "# KNN Model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')  # k can be adjusted\n",
    "knn.fit(X_train, y_train)\n",
    "# Predict Churn\n",
    "y_pred = knn.predict(X_test)\n",
    "# Scatter Plot for Continuous Features\n",
    "df_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "df_test['Churn'] = y_test\n",
    "for feature in numeric_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(df_test[feature], df_test['Churn'], c=y_pred, cmap='coolwarm')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Churn')\n",
    "    plt.title(f\"Scatter Plot: {feature} vs Churn\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# StratifiedKFold for balanced splits\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Random Forest\": rf,\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Neural Networks\": nn,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"KNN\":knn\n",
    "}\n",
    "\n",
    "# Evaluating models\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name} Model:\")\n",
    "\n",
    "    # Cross-validation scores\n",
    "    acc_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    prec_scores = cross_val_score(model, X, y, cv=cv, scoring='precision')\n",
    "    rec_scores = cross_val_score(model, X, y, cv=cv, scoring='recall')\n",
    "    f1_scores = cross_val_score(model, X, y, cv=cv, scoring='f1')\n",
    "    \n",
    "    print(f\"Accuracy: {np.mean(acc_scores):.4f}\")\n",
    "    print(f\"Precision: {np.mean(prec_scores):.4f}\")\n",
    "    print(f\"Recall: {np.mean(rec_scores):.4f}\")\n",
    "    print(f\"F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "    print()\n",
    "\n",
    "# 08-INT\n",
    "\n",
    "# Summarise Results\n",
    "\n",
    "# Add relevant tables or graphs\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shap\n",
    "\n",
    "# Histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_cleaned['TotalCharges'], bins=30, alpha=0.7, label='TotalCharges')\n",
    "plt.hist(df_cleaned['MonthlyCharges'], bins=30, alpha=0.7, label='MonthlyCharges')\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Total and Monthly Charges\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "# Evaluation\n",
    "y_pred = rf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter Plot for Continuous Features\n",
    "df_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "df_test['Churn'] = y_test\n",
    "\n",
    "for feature in numeric_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(df_test[feature], df_test['Churn'], c=df_test['Churn'], cmap='coolwarm')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Churn')\n",
    "    plt.title(f\"Scatter Plot: {feature} vs Churn\")\n",
    "    plt.show()\n",
    "\n",
    "# Learning Curves\n",
    "training_loss = nn.loss_curve_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "feature_importances.plot(kind='bar', title=\"Random Forest Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Matrix\n",
    "corr_matrix = df_cleaned.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluation Metrics\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\"Model\": name, \"Accuracy\": accuracy, \"F1 Score\": f1, \"ROC-AUC\": roc_auc})\n",
    "\n",
    "# Convert to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Visualize the Results\n",
    "plt.figure(figsize=(10, 6))\n",
    "results_df.set_index(\"Model\")[[\"Accuracy\", \"F1 Score\", \"ROC-AUC\"]].plot(kind='bar')\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.show()\n",
    "\n",
    "# SHAP Analysis\n",
    "background_sample = shap.kmeans(X_train, 20)\n",
    "explainer = shap.KernelExplainer(rf.predict_proba, background_sample)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "print(\"SHAP values shape:\", shap_values[1].shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=X.columns)\n",
    "\n",
    "# SHAP Dependency Plot\n",
    "shap.dependence_plot(\"TotalCharges\", shap_values[1], X_test, feature_names=X.columns)\n",
    "\n",
    "# 09-ACT\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Desribe the Action Plan to Implement, Observe and Improve\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c6f5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
